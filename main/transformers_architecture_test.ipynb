{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build transfomer model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def Relu_derivative(x):\n",
    "    return np.where(x>0,1,0)\n",
    "class AddAndNorm():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    def __init__(self,d_model=4,num_heads=2,layer_name=\"MHA\"):\n",
    "        \"\"\"\n",
    "        Create a multi-head attention object, storing the weights of the layer\n",
    "        -- It is necessary to initialize the weights of the model before using it\n",
    "        Parameters:\n",
    "        d_model: int, the dimension of the model\n",
    "        num_heads: int, the number of heads in the model\n",
    "        \"\"\"\n",
    "        \n",
    "        # check if the number of heads is a factor of the model dimension\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.Wq = []\n",
    "        self.Wk = []\n",
    "        self.Wv = []\n",
    "        self.num_heads = num_heads\n",
    "        self.Wo = None\n",
    "        self.d_model_head = d_model // num_heads\n",
    "        self.layer_name=layer_name\n",
    "\n",
    "        \n",
    "        \n",
    "    def init_weights(self,weights_mode=\"random\",weights_q_list=None,weights_k_list=None,weights_v_list=None,weights_o=None):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        weights_mode: str, the mode of the weights initialization, can be \"random\" or \"pretrained\"\n",
    "        weights_q_list: list of numpy arrays, the weights of the query matrix for each head\n",
    "        weights_k_list: list of numpy arrays, the weights of the key matrix for each head\n",
    "        weights_v_list: list of numpy arrays, the weights of the value matrix for each head\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if weights_mode==\"random\":\n",
    "            for _ in range(self.num_heads):\n",
    "                self.Wq.append(np.random.randn(self.d_model,self.d_model_head))\n",
    "                self.Wk.append(np.random.randn(self.d_model,self.d_model_head))\n",
    "                self.Wv.append(np.random.randn(self.d_model,self.d_model_head))\n",
    "                \n",
    "                self.Wo = np.random.randn(self.d_model,self.d_model)\n",
    "                \n",
    "\n",
    "                \n",
    "        elif weights_mode==\"pretrained\":\n",
    "            # check if the weights are provided and if they are of the correct shape\n",
    "            if (weights_q_list is None or weights_k_list is None or \n",
    "                weights_v_list is None or weights_o is None):\n",
    "                raise ValueError(\"weights_q_list, weights_k_list, weights_v_list, and weights_o must be provided for pretrained mode.\")\n",
    "            assert weights_o.shape == (self.d_model,self.d_model)\n",
    "            self.Wo = weights_o\n",
    "            \n",
    "            \n",
    "            for ind in range(self.num_heads):\n",
    "                assert weights_q_list[ind].shape == (self.d_model,self.d_model_head)\n",
    "                assert weights_k_list[ind].shape == (self.d_model,self.d_model_head)\n",
    "                assert weights_v_list[ind].shape == (self.d_model,self.d_model_head)\n",
    "                \n",
    "                \n",
    "                # append the weights and biases to the list\n",
    "                self.Wq.append(weights_q_list[ind])\n",
    "                self.Wk.append(weights_k_list[ind])\n",
    "                self.Wv.append(weights_v_list[ind])\n",
    "\n",
    "                \n",
    "            \n",
    "        elif weights_mode == \"null\":\n",
    "            self.Wo = np.zeros((self.d_model,self.d_model))\n",
    "            self.Bo = np.zeros((1,self.d_model))\n",
    "            for _ in range(self.num_heads):\n",
    "                self.Wq.append(np.zeros((self.d_model,self.d_model_head)))\n",
    "                self.Wk.append(np.zeros((self.d_model,self.d_model_head)))\n",
    "                self.Wv.append(np.zeros((self.d_model,self.d_model_head)))\n",
    "            \n",
    "                \n",
    "    def softmax_single_batch(self,x):\n",
    "        \"\"\"\n",
    "        Compute the softmax of the input matrix x of shape (seq_len,d_model)\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x,axis=1,keepdims=True)) # subtract the max to avoid numerical instability\n",
    "        return exp_x / np.sum(exp_x,axis=1,keepdims=True) # sum along the rows to get the softmax\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        \"\"\"\n",
    "        Compute the softmax of the input matrix x of shape (batch_size,seq_len,d_model)\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x,axis=-1,keepdims=True)) # subtract the max to avoid numerical instability\n",
    "        return exp_x / np.sum(exp_x,axis=-1,keepdims=True) \n",
    "    def scaled_dot_product_attention_single_batch(self,Q,K,V):\n",
    "        \"\"\" \n",
    "        Compute the scaled dot product attention\n",
    "        parameters:\n",
    "        Q: the query matrix of shape (seq_len,d_model)\n",
    "        K: the key matrix of shape (seq_len,d_model)\n",
    "        V: the value matrix of shape (seq_len,d_model)\n",
    "        Returns:\n",
    "        the attention matrix of shape (seq_len,d_model)\n",
    "        \"\"\"\n",
    "        d_k=K.shape[1]\n",
    "        \n",
    "        # compute the dot product\n",
    "        dot_product = np.dot(Q,K.T)\n",
    "        \n",
    "        # scale the dot product\n",
    "        scaled_dot_product = dot_product / np.sqrt(d_k)\n",
    "        # apply the softmax\n",
    "        attention_matrix = self.softmax_single_batch(scaled_dot_product)\n",
    "\n",
    "        #multiply the attention matrix by the value matrix\n",
    "        attention_matrix = np.dot(attention_matrix,V)\n",
    "        \n",
    "        return attention_matrix\n",
    "    def scaled_dot_product_attention(self,Q,K,V):\n",
    "        \"\"\" \n",
    "        Compute the scaled dot product attention\n",
    "        parameters:\n",
    "        Q: the query matrix of shape (batch_size, seq_len, d_model)\n",
    "        K: the key matrix of shape (batch_size, seq_len, d_model)\n",
    "        V: the value matrix of shape (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        the attention matrix of shape (seq_len,d_model)\n",
    "        \"\"\"\n",
    "        d_k=K.shape[1]\n",
    "        \n",
    "        # compute the dot product\n",
    "        dot_product = np.matmul(Q, K.transpose(0, 2, 1)) \n",
    "        \n",
    "        # scale the dot product\n",
    "        scaled_dot_product = dot_product / np.sqrt(d_k)\n",
    "        # apply the softmax\n",
    "        attention_weights  = self.softmax(scaled_dot_product)\n",
    "\n",
    "        #multiply the attention matrix by the value matrix\n",
    "        attention_output = np.matmul(attention_weights, V)\n",
    "        \n",
    "        return attention_output\n",
    "    \n",
    "    def scaled_dot_product_masked_attention(self,Q,K,V):\n",
    "        \"\"\" \n",
    "        Compute the scaled dot product attention\n",
    "        parameters:\n",
    "        Q: the query matrix of shape (batch_size, seq_len, d_model)\n",
    "        K: the key matrix of shape (batch_size, seq_len, d_model)\n",
    "        V: the value matrix of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "        the attention matrix of shape (seq_len,d_model)\n",
    "        \"\"\"\n",
    "        d_k=K.shape[1]\n",
    "        \n",
    "        # compute the dot product\n",
    "        dot_product = np.matmul(Q, K.transpose(0, 2, 1)) \n",
    "        # scale the dot product\n",
    "        scaled_dot_product = dot_product / np.sqrt(d_k)\n",
    "        # apply the mask\n",
    "        \n",
    "        mask = np.triu(np.ones((Q.shape[1],K.shape[1])),k=1)\n",
    "        mask = mask == 0\n",
    "        scaled_dot_product = np.where(mask,scaled_dot_product,-np.inf)\n",
    "        \n",
    "        \n",
    "        # apply the softmax\n",
    "        attention_weights  = self.softmax(scaled_dot_product)\n",
    "\n",
    "        \n",
    "        #multiply the attention matrix by the value matrix\n",
    "        attention_output = np.matmul(attention_weights, V)\n",
    "        return attention_output\n",
    "    \n",
    "    def compute_MHA(self,inputs):\n",
    "        \"\"\"\n",
    "        Compute the multi-head attention\n",
    "        Parameters:\n",
    "        \n",
    "        inputs: the input matrix of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "        the output of the multi-head attention layer of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        attention_matrix_list=[]\n",
    "        \n",
    "        for ind in range(self.num_heads):\n",
    "            Wq = self.Wq[ind]\n",
    "            Wk = self.Wk[ind]\n",
    "            Wv = self.Wv[ind]\n",
    "            \n",
    "            Q_head = np.dot(inputs,Wq)\n",
    "            K_head = np.dot(inputs,Wk)\n",
    "            V_head = np.dot(inputs,Wv)\n",
    "            \n",
    "            # compute the scaled dot product attention\n",
    "            attention_matrix_head = self.scaled_dot_product_attention(Q_head,K_head,V_head)\n",
    "\n",
    "            attention_matrix_list.append(attention_matrix_head)\n",
    "        \n",
    "        # concatenate the attention matrices\n",
    "        attention_matrix = np.concatenate(attention_matrix_list,axis=-1)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # apply the final linear transformation Wo\n",
    "        attention_matrix = np.dot(attention_matrix,self.Wo) \n",
    "        return attention_matrix\n",
    "    def compute_MHA_single_batch(self,inputs):\n",
    "        \"\"\"\n",
    "        Compute the multi-head attention\n",
    "        Parameters:\n",
    "        \n",
    "        inputs: the input matrix of shape (seq_len,d_model)\n",
    "        \n",
    "        Returns:\n",
    "        the output of the multi-head attention layer of shape (seq_len,d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        attention_matrix_list=[]\n",
    "        \n",
    "        for ind in range(self.num_heads):\n",
    "\n",
    "            Wq = self.Wq[ind]\n",
    "            Wk = self.Wk[ind]\n",
    "            Wv = self.Wv[ind]\n",
    "            \n",
    "            Q_head = np.dot(inputs,Wq)\n",
    "            K_head = np.dot(inputs,Wk)\n",
    "            V_head = np.dot(inputs,Wv)\n",
    "\n",
    "            # compute the scaled dot product attention\n",
    "            attention_matrix_head = self.scaled_dot_product_attention_single_batch(Q_head,K_head,V_head)\n",
    "\n",
    "            attention_matrix_list.append(attention_matrix_head)\n",
    "        \n",
    "        # concatenate the attention matrices\n",
    "        attention_matrix = np.concatenate(attention_matrix_list,axis=1)\n",
    "        \n",
    "        \n",
    "        # apply the final linear transformation Wo\n",
    "        attention_matrix = np.dot(attention_matrix,self.Wo) \n",
    "        return attention_matrix\n",
    "        \n",
    "class FeedForward():\n",
    "    def __init__(self,d_model=4,max_seq_len=100,units=2048,name=\"FF_layer_X\"):\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.weights1 = None\n",
    "        self.weights2 = None\n",
    "        self.bias1 = None\n",
    "        self.bias2 = None\n",
    "        self.name = name\n",
    "        self.units = units # the number of units in the hidden layer\n",
    "    def init_weights(self,weights_mode=\"random\",weights1=None,weights2=None,bias1=None,bias2=None):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        weights_mode: str, the mode of the weights initialization, can be \"random\" or \"pretrained\"\n",
    "        weights1: numpy array, the weights of the first layer\n",
    "        weights2: numpy array, the weights of the second layer\n",
    "        bias1: numpy array, the bias of the first layer\n",
    "        bias2: numpy array, the bias of the second layer\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if weights_mode==\"random\":\n",
    "            self.weights1 = np.random.randn(self.d_model,self.units)\n",
    "            self.weights2 = np.random.randn(self.units,self.d_model)\n",
    "            self.bias1 = np.random.randn(1,self.units)\n",
    "            self.bias2 = np.random.randn(1,self.d_model)\n",
    "        elif weights_mode==\"pretrained\":\n",
    "            # check if the weights are provided and if they are of the correct shape\n",
    "            if (weights1 is None or weights2 is None or \n",
    "                bias1 is None or bias2 is None):\n",
    "                raise ValueError(\"weights1, weights2, bias1, and bias2 must be provided for pretrained mode.\")\n",
    "            assert weights1.shape == (self.d_model,self.units)\n",
    "            assert weights2.shape == (self.units,self.d_model)\n",
    "            assert bias1.shape == (1,self.units)\n",
    "            assert bias2.shape == (1,self.d_model)\n",
    "            \n",
    "            self.weights1 = weights1\n",
    "            self.weights2 = weights2\n",
    "            self.bias1 = bias1\n",
    "            self.bias2 = bias2\n",
    "            \n",
    "        elif weights_mode == \"null\":\n",
    "            self.weights1 = np.zeros((self.d_model,self.units))\n",
    "            self.weights2 = np.zeros((self.units,self.d_model))\n",
    "            self.bias1 = np.zeros((1,self.units))\n",
    "            self.bias2 = np.zeros((1,self.d_model))\n",
    "    def forward_batch_sequence(self,inputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs: numpy array, dimensions (batch_size, max_seq_len, d_model), the input data\n",
    "        \n",
    "        Returns:\n",
    "        numpy array, dimensions (batch_size, max_seq_len, d_model), the output of the layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # X shape: (batch_size, max_seq_len, d_model)\n",
    "        self.batch_size, self.max_seq_len, _ = inputs.shape\n",
    "        # Reshape X to 2D for easier computation\n",
    "        X_reshaped = inputs.reshape(-1, self.d_model)\n",
    "        # First layer\n",
    "        self.Z1 = np.dot(X_reshaped, self.weights1) + self.bias1\n",
    "        \n",
    "        # apply the activation function \n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU activation\n",
    "            \n",
    "        # Second layer\n",
    "        Z2 = np.dot(self.A1, self.weights2) + self.bias2\n",
    "            \n",
    "        # Reshape output back to 3D\n",
    "        return Z2.reshape(self.batch_size, self.max_seq_len, self.d_model)\n",
    "        \n",
    "    def backprop_batch_sequence(self, dA2):\n",
    "        pass\n",
    "    \n",
    "    def update_weights(self,learning_rate=0.01):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerModel():\n",
    "    # model parameters\n",
    "    def __init__(self,embedding_dict_path,vocab_size=1000,d_model=4): \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model # the dimension of the embedding space, aka d_model\n",
    "        self.max_seq_len = 100\n",
    "        # read the embedding dictionary\n",
    "        # the file is a csv file with the first column as the word and the rest as the embedding vector\n",
    "        embedding_dict={}\n",
    "        with open(embedding_dict_path,'r') as f:\n",
    "            # create a dictionary with the word as the key and the embedding vector as the value\n",
    "            # force the value to float64\n",
    "            for line in f:\n",
    "                elements=line.strip().split(',')\n",
    "                word=elements[0]\n",
    "                vector=np.array(elements[1:],dtype=np.float64)\n",
    "                embedding_dict[word] = vector\n",
    "        self.embedding_dict = embedding_dict\n",
    "        \n",
    "        self.MultiHeadAttention_obj_list = []\n",
    "        self.FeedForward_obj_list = []\n",
    "\n",
    "    \n",
    "    def inputs_layer(self,inputs): \n",
    "        \"\"\"\n",
    "        \n",
    "        input is a list of valid strings, return a tensor representation of the input in the embedding space\n",
    "        \n",
    "        iterate through the list of strings and convert them to numpy arrays through the embedding dictionary\n",
    "        \n",
    "        \"\"\"\n",
    "        embeddings = [self.embedding_dict[word] for word in inputs]\n",
    "        tensor = np.array(embeddings)\n",
    "        return tensor\n",
    "    \n",
    "    def positional_encoding(self,seq_len):\n",
    "        \"\"\"\n",
    "        create a positional encoding for the input sequence ( it is independent of the input sequence.\n",
    "        However, it is dependent on the length of the sequence) \n",
    "        \n",
    "        Parameters: \n",
    "        seq_len: the length of the sequence\n",
    "        Returns : \n",
    "        Tensor of shape (seq_len,d_model) \n",
    "        \"\"\"\n",
    "        pos_encodings = np.zeros((seq_len, self.d_model))\n",
    "\n",
    "\n",
    "        positions = np.arange(seq_len)[:, np.newaxis]\n",
    "        dimensions = np.arange(self.d_model)[np.newaxis, :]\n",
    "\n",
    "\n",
    "        # Compute angles, integer division of indices by 2 is used to fix a pair wise frequency (same frequency for even and odd indices)\n",
    "        angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float64(self.d_model))\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        # Apply sin and cos to odd and even indices\n",
    "        pos_encodings[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        pos_encodings[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        return pos_encodings\n",
    "    \n",
    "    def add_multi_head_attention(self,num_heads=8,weights_mode=\"random\",layer_name=\"MHA\",weights_q_list=None,weights_k_list=None,weights_v_list=None,weights_o=None):\n",
    "        \"\"\"\n",
    "        Create and add a multi-head attention layer\n",
    "        \n",
    "        \"\"\"\n",
    "        MHA= MultiHeadAttention(d_model=self.d_model,num_heads=num_heads,layer_name=layer_name)\n",
    "        MHA.init_weights(weights_mode=weights_mode,weights_q_list=weights_q_list,weights_k_list=weights_k_list,weights_v_list=weights_v_list,weights_o=weights_o)\n",
    "        self.MultiHeadAttention_obj_list.append(MHA)\n",
    "    \n",
    "    \n",
    "    def LayerNormalization(self,inputs,epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Apply layer normalization to the input tensor\n",
    "        Parameters:\n",
    "        inputs: the input tensor of shape (batch_size,seq_len,d_model)\n",
    "        epsilon: a small value to avoid division by zero\n",
    "        Returns:\n",
    "        Normalized tensor of shape (batch_size,seq_len,d_model)\n",
    "        \n",
    "        We could also consider adding 2 learnable parameters to the layer normalization\n",
    "        gamma and beta\n",
    "        \"\"\"\n",
    "        mean = np.mean(inputs,axis=-1,keepdims=True)\n",
    "        std = np.std(inputs,axis=-1,keepdims=True)\n",
    "        normalized_inputs = (inputs - mean) / (std + epsilon)\n",
    "        return normalized_inputs\n",
    "    \n",
    "    def AddAndNorm(self,inputs,sublayer_output,epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Add the input tensor to the sublayer output and apply layer normalization\n",
    "        Parameters:\n",
    "        inputs: the input tensor of shape (batch_size,seq_len,d_model)\n",
    "        sublayer_output: the output of the sublayer of shape (batch_size,seq_len,d_model)\n",
    "        Returns:\n",
    "        Normalized sum tensor of shape (batch_size,seq_len,d_model)\n",
    "        \"\"\"\n",
    "        return self.LayerNormalization(inputs + sublayer_output,epsilon=epsilon)\n",
    "    def encoder(self,inputs):\n",
    "        \"\"\"\n",
    "        1- create the input layer\n",
    "        2- create the positional encoding\n",
    "        3- sum the input layer and the positional encoding\n",
    "        etc\n",
    "        \"\"\"\n",
    "        # create the input layer / embeddings\n",
    "        inputs_embeddings = self.inputs_layer(inputs)\n",
    "        pos_encodings = self.positional_encoding(len(inputs))\n",
    "        encoder_input = inputs_embeddings + pos_encodings\n",
    "        \n",
    "        # pass the input through the multi-head attention layers\n",
    "        \n",
    "        return encoder_input\n",
    "        \n",
    "     \n",
    "    def decoder(self):\n",
    "        pass\n",
    "    def forward(self):\n",
    "        pass\n",
    "    def train(self):\n",
    "        pass\n",
    "    def inference(self):\n",
    "        pass\n",
    "    def save(self):\n",
    "        pass\n",
    "    def load(self):\n",
    "        pass\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self,seqlen,d_model,batch_size):\n",
    "        self.seq_len = seqlen\n",
    "        self.d_model = d_model\n",
    "        self.batch_size = batch_size\n",
    "        self._positional_encoding = None  # Cache for the positional encoding\n",
    "    def positional_encoding(self):\n",
    "        \"\"\"\n",
    "        create a positional encoding for the input sequence ( it is independent of the input sequence.\n",
    "        However, it is dependent on the length of the sequence) \n",
    "        This is computed once and then repeated for the batch size.\n",
    "        \n",
    "\n",
    "        Returns : \n",
    "        Tensor of shape (batch_size,seq_len,d_model) \n",
    "        \"\"\" \n",
    "        if self._positional_encoding is None:\n",
    "            # Compute positional encoding for a single sequence\n",
    "            pos_encoding = np.zeros((self.seq_len, self.d_model))\n",
    "            positions = np.arange(self.seq_len)[:, np.newaxis]\n",
    "            dimensions = np.arange(self.d_model)[np.newaxis, :]\n",
    "            angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float64(self.d_model))\n",
    "            angle_rads = positions * angle_rates\n",
    "\n",
    "            pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "            pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "            # Repeat for batch size and cache\n",
    "            self._positional_encoding = np.repeat(pos_encoding[np.newaxis, :, :], self.batch_size, axis=0)\n",
    "\n",
    "        return self._positional_encoding\n",
    "    def positional_encoding_single_sequence(self):\n",
    "        \"\"\"\n",
    "        create a positional encoding for the input sequence ( it is independent of the input sequence.\n",
    "        However, it is dependent on the length of the sequence) \n",
    "        \n",
    "        Parameters: \n",
    "        seq_len: the length of the sequence\n",
    "        Returns : \n",
    "        Tensor of shape (seq_len,d_model) \n",
    "        \"\"\"\n",
    "        pos_encodings = np.zeros((self.seq_len, self.d_model))\n",
    "\n",
    "\n",
    "        positions = np.arange(self.seq_len)[:, np.newaxis]\n",
    "        dimensions = np.arange(self.d_model)[np.newaxis, :]\n",
    "\n",
    "\n",
    "        # Compute angles, integer division of indices by 2 is used to fix a pair wise frequency (same frequency for even and odd indices)\n",
    "        angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float64(self.d_model))\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        # Apply sin and cos to odd and even indices\n",
    "        pos_encodings[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        pos_encodings[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        return pos_encodings\n",
    "\n",
    "class InputEmbedding:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def input_embedding(self,inputs,embedding_dict):\n",
    "        \"\"\"\n",
    "        Convert the input batch of strings to its corresponding embedding vector\n",
    "        \n",
    "        parameters:\n",
    "        inputs: list of strings, the input batch, the length of the strings is seq_len\n",
    "        Returns:\n",
    "        numpy array of shape (batch_size,seq_len,d_model)\n",
    "        \"\"\"\n",
    "        batch_sequences = []\n",
    "        for input in inputs:\n",
    "            single_sequence = []\n",
    "            for word in input:\n",
    "                if word not in embedding_dict:\n",
    "                    raise ValueError(f\"{word} not found in the embedding dictionary\")\n",
    "                single_sequence.append(embedding_dict[word])\n",
    "            batch_sequences.append(single_sequence)\n",
    "        # return a tensor of shape (batch_size,seq_len,d_model)\n",
    "        return np.array(batch_sequences)\n",
    "        \n",
    "class OutputEmbedding:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "class Encoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "class Decoder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/word_embedding.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test the model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m transformer\u001b[38;5;241m=\u001b[39m\u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/word_embedding.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# test inputs_layer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m transformer\u001b[38;5;241m.\u001b[39minputs_layer([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 320\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[1;34m(self, embedding_dict_path, vocab_size, d_model)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# read the embedding dictionary\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# the file is a csv file with the first column as the word and the rest as the embedding vector\u001b[39;00m\n\u001b[0;32m    319\u001b[0m embedding_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding_dict_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# create a dictionary with the word as the key and the embedding vector as the value\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# force the value to float64\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m    324\u001b[0m         elements\u001b[38;5;241m=\u001b[39mline\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/word_embedding.csv'"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "transformer=TransformerModel('data/word_embedding.csv')\n",
    "# test inputs_layer\n",
    "\n",
    "transformer.inputs_layer(['in','is','the'])\n",
    "\n",
    "# test positional_encoding\n",
    "\n",
    "transformer.positional_encoding(10)\n",
    "transformer.encoder(['in','is','the'])\n",
    "\n",
    "# test softmax\n",
    "MHA=MultiHeadAttention()\n",
    "x=np.array([[1,2,3],[0,0,0],[-1,0,1],[1,1,1],[0,1,0]])\n",
    "MHA.softmax(x)\n",
    "\n",
    "# test single head attention\n",
    "\n",
    "MHA=MultiHeadAttention()\n",
    "Q_test=np.array([[[1,1,1],[1,1,1],[1,1,1]],[[1,1,1],[1,1,1],[1,1,1]]])\n",
    "K_test=np.array([[[5,5,5],[5,5,5],[5,5,5]],[[5,5,5],[5,5,5],[5,5,5]]])\n",
    "V_test=np.array([[[3,3,3],[3,3,3],[3,3,3]],[[3,3,3],[3,3,3],[3,3,3]]])\n",
    "print(Q_test.shape,K_test.shape,V_test.shape)\n",
    "MHA.scaled_dot_product_attention(Q_test,K_test,V_test)\n",
    "\n",
    "\n",
    "# test multi head attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]],\n",
       "\n",
       "       [[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test multi head masked attention\n",
    "MHA=MultiHeadAttention()\n",
    "Q_test=np.array([[[0,-1,5],[1,5,1],[1,-7,1]],[[1,1,1],[1,1,1],[1,1,1]]])\n",
    "K_test=np.array([[[2,4,-1],[0,-1,5],[5,5,5]],[[5,5,5],[5,5,5],[5,5,5]]])\n",
    "V_test=np.array([[[3,3,3],[3,3,3],[3,3,3]],[[3,3,3],[3,3,3],[3,3,3]]])\n",
    "MHA.scaled_dot_product_masked_attention(Q_test,K_test,V_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_test=np.triu(np.ones((Q_test.shape[1],K_test.shape[1])),k=1)\n",
    "mask_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.09003057, 0.24472847, 0.66524096],\n",
       "        [0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.09003057, 0.24472847, 0.66524096],\n",
       "        [0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.21194156, 0.57611688, 0.21194156]],\n",
       "\n",
       "       [[0.09003057, 0.24472847, 0.66524096],\n",
       "        [0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.09003057, 0.24472847, 0.66524096],\n",
       "        [0.33333333, 0.33333333, 0.33333333],\n",
       "        [0.21194156, 0.57611688, 0.21194156]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test softmax\n",
    "MHA=MultiHeadAttention()\n",
    "x=np.array([\n",
    "    [[1,2,3],[0,0,0],[-1,0,1],[1,1,1],[0,1,0] ],[[1,2,3],[0,0,0],[-1,0,1],[1,1,1],[0,1,0]]])\n",
    "print(x.shape)\n",
    "MHA.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.79869662,  5.59041132, -1.58126427,  1.32009245],\n",
       "       [ 0.69047119,  1.97661677, -1.54997988,  2.45496703],\n",
       "       [ 0.83407547,  3.93658877, -3.49740973,  5.78633289],\n",
       "       [ 4.83756115,  5.69753497, -1.45059594,  1.13045698],\n",
       "       [ 3.97021874,  4.5865725 , -1.27075659,  1.20886365]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test a single batch of multi head attention\n",
    "MHA=MultiHeadAttention(d_model=4,num_heads=2)\n",
    "MHA.init_weights(weights_mode=\"random\")\n",
    "x=np.array([[1,2,3,4],[0,0,0,0],[-1,0,0,1],[1,1,1,1],[1,0,1,0]])\n",
    "\n",
    "MHA.compute_MHA_single_batch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n",
      "(2, 4, 4)\n",
      "[[[-5.68512693 -1.40290013 -1.09531664 11.79513054]\n",
      "  [-2.62975504 -1.43059249 -0.89214632  4.31637048]\n",
      "  [-2.92025685 -2.724588   -1.03785132  2.91894725]\n",
      "  [-4.76266657 -1.1561036  -1.02551252  9.92855121]]\n",
      "\n",
      " [[-5.68512693 -1.40290013 -1.09531664 11.79513054]\n",
      "  [-2.62975504 -1.43059249 -0.89214632  4.31637048]\n",
      "  [-2.92025685 -2.724588   -1.03785132  2.91894725]\n",
      "  [-4.76266657 -1.1561036  -1.02551252  9.92855121]]]\n"
     ]
    }
   ],
   "source": [
    "# test multi head attention with a batch of inputs\n",
    "\n",
    "x=np.array([\n",
    "    [[1,2,3,4],[0,0,0,0],[-1,0,0,1],[1,1,1,1]],[[1,2,3,4],[0,0,0,0],[-1,0,0,1],[1,1,1,1]]])\n",
    "print(x.shape)\n",
    "\n",
    "MHA=MultiHeadAttention(d_model=4,num_heads=2)\n",
    "MHA.init_weights(weights_mode=\"random\")\n",
    "answer=MHA.compute_MHA(x)\n",
    "print(answer.shape)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape (4, 2) b shape (2, 2, 4)\n",
      "[[[ 50  60]\n",
      "  [114 140]]\n",
      "\n",
      " [[178 220]\n",
      "  [242 300]]]\n",
      "(2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# generate a matrix of shape 4,2 and another of shape 2 4 4 and multiply them\n",
    "\n",
    "a=np.array([[1,2],[3,4],[5,6],[7,8]])\n",
    "b=np.array([[[1,2,3,4],[5,6,7,8]],[[9,10,11,12],[13,14,15,16]]])\n",
    "print(\"a shape\",a.shape,\"b shape\",b.shape)\n",
    "answer=np.dot(b,a)\n",
    "print(answer)\n",
    "print(answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12.78858908, -233.84729112,   98.4903025 ,  -41.54592723],\n",
       "       [  28.18929749,    0.82212271,   -5.31149212,   12.68068014],\n",
       "       [  39.9695274 ,  -15.04990048,   30.53381561,   -9.88624043],\n",
       "       [   5.77393591,  -65.29353805,    9.35475249,    2.85573199],\n",
       "       [ -26.88110994,   -9.49735534,  -24.20271546,    0.69148265]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the feed forward layer\n",
    "\n",
    "FF=FeedForward(d_model=4,max_seq_len=100,units=1024,name=\"FF_layer_X\")\n",
    "\n",
    "FF.init_weights(weights_mode=\"random\")\n",
    "x=np.array([[1,2,3,4],[0,0,0,0],[-1,0,0,1],[1,1,1,1],[1,0,1,0]])\n",
    "FF.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (2, 5, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-1.34163959, -0.4472132 ,  0.4472132 ,  1.34163959],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-1.41421156,  0.        ,  0.        ,  1.41421156],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.999998  , -0.999998  ,  0.999998  , -0.999998  ]],\n",
       "\n",
       "       [[-1.34163959, -0.4472132 ,  0.4472132 ,  1.34163959],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-1.41421156,  0.        ,  0.        ,  1.41421156],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.999998  , -0.999998  ,  0.999998  , -0.999998  ]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test layer normalization\n",
    "x=np.array([[[1,2,3,4],[0,0,0,0],[-1,0,0,1],[1,1,1,1],[1,0,1,0]],[[1,2,3,4],[0,0,0,0],[-1,0,0,1],[1,1,1,1],[1,0,1,0]]])\n",
    "print(\"x shape\",x.shape)\n",
    "TF=TransformerModel('data/word_embedding.csv')\n",
    "TF.LayerNormalization(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
