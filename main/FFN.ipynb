{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mse_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error loss between the true and predicted values (the cost function)\n",
    "    \"\"\"\n",
    "    return np.mean(np.square(y_true-y_pred))\n",
    "    \n",
    "\n",
    "def derivative_mse_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the mean squared error loss function with respect to the predicted values\n",
    "    input:\n",
    "    y_true: numpy array, dimensions (batch_size,seq_len,d_model), the true values\n",
    "    y_pred: numpy array, dimensions (batch_size,seq_len,d_model), the predicted values\n",
    "    output:\n",
    "    numpy array, dimensions (batch_size,seq_len,d_model), the derivative of the loss function with respect to the predicted values \n",
    "    \"\"\"\n",
    "    #return 2*(y_pred-y_true)/y_true.shape[0] \n",
    "    #return 2*(y_pred-y_true)\n",
    "    # print numpy abs vals and their sum\n",
    "    #print(2*np.abs(y_pred-y_true).sum()/y_true.size)\n",
    "    return 2*(y_pred-y_true)/y_true.size\n",
    "    \n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def Relu_derivative(x):\n",
    "    return np.where(x>0,1,0)\n",
    "class TransfomerFF:\n",
    "    def __init__(self,d_model=4,max_seq_len=100,units=2048,batch_size=32,name=\"FF_layer_X\"):\n",
    "            self.d_model = d_model\n",
    "            self.max_seq_len = max_seq_len\n",
    "            self.batch_size = batch_size\n",
    "            self.weights1 = None\n",
    "            self.weights2 = None\n",
    "            self.bias1 = None\n",
    "            self.bias2 = None\n",
    "            self.name = name\n",
    "            self.units = units # the number of units in the hidden layer\n",
    "    def init_weights(self,weights_mode=\"random\",weights1=None,weights2=None,bias1=None,bias2=None):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        weights_mode: str, the mode of the weights initialization, can be \"random\" or \"pretrained\"\n",
    "        weights1: numpy array, the weights of the first layer\n",
    "        weights2: numpy array, the weights of the second layer\n",
    "        bias1: numpy array, the bias of the first layer\n",
    "        bias2: numpy array, the bias of the second layer\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if weights_mode==\"random\":\n",
    "            self.weights1 = np.random.randn(self.d_model,self.units)*0.01\n",
    "            self.weights2 = np.random.randn(self.units,self.d_model)*0.01\n",
    "            self.bias1 = np.zeros((1, self.units)) #np.random.randn(1,self.units)\n",
    "            self.bias2 =  np.zeros((1, self.d_model)) #  np.random.randn(1,self.d_model)\n",
    "        elif weights_mode==\"pretrained\":\n",
    "            # check if the weights are provided and if they are of the correct shape\n",
    "            if (weights1 is None or weights2 is None or \n",
    "                bias1 is None or bias2 is None):\n",
    "                raise ValueError(\"weights1, weights2, bias1, and bias2 must be provided for pretrained mode.\")\n",
    "            assert weights1.shape == (self.d_model,self.units)\n",
    "            assert weights2.shape == (self.units,self.d_model)\n",
    "            assert bias1.shape == (1,self.units)\n",
    "            assert bias2.shape == (1,self.d_model)\n",
    "            \n",
    "            self.weights1 = weights1\n",
    "            self.weights2 = weights2\n",
    "            self.bias1 = bias1\n",
    "            self.bias2 = bias2\n",
    "            \n",
    "        elif weights_mode == \"null\":\n",
    "            self.weights1 = np.zeros((self.d_model,self.units))\n",
    "            self.weights2 = np.zeros((self.units,self.d_model))\n",
    "            self.bias1 = np.zeros((1,self.units))\n",
    "            self.bias2 = np.zeros((1,self.d_model))\n",
    "    def forward_single_sequence(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: numpy array, dimensions ( max_seq_len, d_model), the input data\n",
    "        \n",
    "        Returns:\n",
    "        numpy array, dimensions ( max_seq_len, d_model), the output of the layer\n",
    "        \"\"\"\n",
    "        z1 = np.dot(inputs,self.weights1) + self.bias1\n",
    "        a1 = Relu(z1)\n",
    "        z2 = np.dot(a1,self.weights2) + self.bias2\n",
    "        return z2\n",
    "    def forward_batch_sequence(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: numpy array, dimensions (batch_size, max_seq_len, d_model), the input data\n",
    "        \n",
    "        Returns:\n",
    "        numpy array, dimensions (batch_size, max_seq_len, d_model), the output of the layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # X shape: (batch_size, max_seq_len, d_model)\n",
    "        self.batch_size, self.max_seq_len, _ = inputs.shape\n",
    "        # Reshape X to 2D for easier computation\n",
    "        X_reshaped = inputs.reshape(-1, self.d_model)\n",
    "        # First layer\n",
    "        self.Z1 = np.dot(X_reshaped, self.weights1) + self.bias1\n",
    "        \n",
    "        # apply the activation function \n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU activation\n",
    "            \n",
    "        # Second layer\n",
    "        Z2 = np.dot(self.A1, self.weights2) + self.bias2\n",
    "            \n",
    "        # Reshape output back to 3D\n",
    "        return Z2.reshape(self.batch_size, self.max_seq_len, self.d_model)\n",
    "    \n",
    "    def backprop_single_sequence(self,inputs,targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs: numpy array, dimensions (seq_len,d_model), the input data\n",
    "        targets: numpy array, dimensions (seq_len,d_model), the target values\n",
    "        \"\"\"\n",
    "        \n",
    "        # create the gradient arrays\n",
    "        grad_weights1 = np.zeros_like(self.weights1)\n",
    "        grad_weights2 = np.zeros_like(self.weights2)\n",
    "        grad_bias1 = np.zeros_like(self.bias1)\n",
    "        grad_bias2 = np.zeros_like(self.bias2)\n",
    "        \n",
    "        # feed forward pass\n",
    "        \n",
    "        z_layer_1 = np.dot(inputs,self.weights1) + self.bias1\n",
    "        a_layer_1 = Relu(z_layer_1)\n",
    "        \n",
    "        z_layer_2 = np.dot(a_layer_1,self.weights2) + self.bias2\n",
    "\n",
    "        # delta is gradient of the cost with respect to z at the output layer ( using BP1 formula)\n",
    "        \n",
    "        delta=derivative_mse_loss(targets,z_layer_2) \n",
    "        \n",
    "        grad_weights2 = np.matmul(a_layer_1.T,delta) \n",
    "        grad_bias2 = delta.sum(axis=0,keepdims=True)\n",
    "        \n",
    "        delta = np.matmul(delta,self.weights2.T) * Relu_derivative(z_layer_1)\n",
    "        grad_weights1 = np.matmul(inputs.T,delta)\n",
    "        grad_bias1 = delta.sum(axis=0,keepdims=True)\n",
    "        \n",
    "        return [grad_weights1,grad_weights2,grad_bias1,grad_bias2]\n",
    "        \n",
    "    def backprop_batch_sequence(self,inputs,targets):\n",
    "        \"\"\"\n",
    "        inputs: numpy array, dimensions (batch_size,seq_len,d_model), the input data\n",
    "        targets: numpy array, dimensions (batch_size,seq_len,d_model), the target values\n",
    "        \"\"\"\n",
    "        grad_weights1 = np.zeros((inputs.shape[0],self.weights1.shape[0],self.weights1.shape[1]))\n",
    "        grad_weights2 = np.zeros((inputs.shape[0],self.weights2.shape[0],self.weights2.shape[1]))\n",
    "        grad_bias1 = np.zeros((inputs.shape[0],self.bias1.shape[0],self.bias1.shape[1]))\n",
    "        grad_bias2 = np.zeros((inputs.shape[0],self.bias2.shape[0],self.bias2.shape[1]))\n",
    "        \n",
    "        # feed forward pass\n",
    "        \n",
    "        z_layer_1 = np.dot(inputs,self.weights1) + self.bias1\n",
    "        a_layer_1 = Relu(z_layer_1)\n",
    "        \n",
    "        z_layer_2 = np.dot(a_layer_1,self.weights2) + self.bias2\n",
    "        \n",
    "         # delta is gradient of the cost with respect to z at the output layer ( using BP1 formula)\n",
    "        \n",
    "        delta=derivative_mse_loss(targets,z_layer_2) \n",
    "\n",
    "        \n",
    "        grad_weights2 = np.matmul(a_layer_1.transpose(0,2,1),delta) \n",
    "        grad_bias2 = delta.sum(axis=1,keepdims=True)\n",
    "        \n",
    "        delta = np.dot(delta,self.weights2.T) * Relu_derivative(z_layer_1)\n",
    "        \n",
    "        grad_weights1 = np.matmul(inputs.transpose(0,2,1),delta)\n",
    "        grad_bias1 = delta.sum(axis=1,keepdims=True)\n",
    "        \n",
    "        return [grad_weights1,grad_weights2,grad_bias1,grad_bias2]\n",
    "    \n",
    "    def update_weights_batch_sequence(self,parameters,learning_rate=0.01):\n",
    "        \n",
    "        grad_weights1 = parameters[0].mean(axis=0)  # Average across the batch dimension\n",
    "        grad_weights2 = parameters[1].mean(axis=0)\n",
    "        grad_bias1 = parameters[2].mean(axis=0)\n",
    "        grad_bias2 = parameters[3].mean(axis=0)\n",
    "        # Update weights and biases\n",
    "        self.weights1 -= learning_rate * grad_weights1\n",
    "        self.weights2 -= learning_rate * grad_weights2\n",
    "        self.bias1 -= learning_rate * grad_bias1\n",
    "        self.bias2 -= learning_rate * grad_bias2\n",
    "        \n",
    "    def update_weights_single_sequence(self,parameters,learning_rate=0.01):\n",
    "        \n",
    "        self.weights1 -= learning_rate*parameters[0]\n",
    "        self.weights2 -= learning_rate*parameters[1]\n",
    "        self.bias1 -= learning_rate*parameters[2]\n",
    "        self.bias2 -= learning_rate*parameters[3]\n",
    "    \n",
    "    def train_single_sequence(self,x,y,epoch=1001,learning_rate=0.1):\n",
    "        \n",
    "        # x: numpy array, dimensions (seq_len,d_model), the input data\n",
    "        loss_to_epoch_history=[]\n",
    "        print_epoch= epoch//10\n",
    "        for epoch in range(epoch):\n",
    "            pred=self.forward_single_sequence(x)\n",
    "            loss=mse_loss(y,pred)\n",
    "            loss_to_epoch_history.append((epoch,loss))\n",
    " \n",
    "            if epoch% print_epoch==0:\n",
    "                print(f\"Epoch:{epoch}, Loss:{loss}\")\n",
    "            params=self.backprop_single_sequence(x,y)\n",
    "            self.update_weights_single_sequence(params,learning_rate)\n",
    "        plt.plot(*zip(*loss_to_epoch_history))\n",
    "        plt.show()\n",
    "        return [self.weights1,self.weights2,self.bias1,self.bias2],loss_to_epoch_history\n",
    "    \n",
    "    \n",
    "    def train_batch_sequence(self,x,y,epoch=1001,learning_rate=0.1):\n",
    "        \n",
    "        # x: numpy array, dimensions (batch_size,seq_len,d_model), the input data\n",
    "        loss_to_epoch_history=[]\n",
    "        print_epoch= epoch//10\n",
    "        for epoch in range(epoch):\n",
    "            pred=self.forward_batch_sequence(x)\n",
    "            loss=mse_loss(y,pred)\n",
    "            loss_to_epoch_history.append((epoch,loss))\n",
    "            if epoch% print_epoch==0:\n",
    "                print(f\"Epoch:{epoch}, Loss:{loss}\")\n",
    "            params=self.backprop_batch_sequence(x,y)\n",
    "            self.update_weights_batch_sequence(params,learning_rate)\n",
    "        plt.plot(*zip(*loss_to_epoch_history))\n",
    "        plt.show()\n",
    "        return [self.weights1,self.weights2,self.bias1,self.bias2],loss_to_epoch_history\n",
    "\n",
    "#### Test Batch Sequence ####\n",
    "print(\"Test batch sequence\")\n",
    "\n",
    "d_model=20\n",
    "max_seq_len=100\n",
    "units=512\n",
    "learning_rate=1\n",
    "batch_size=4\n",
    "num_epochs=1001\n",
    "\n",
    "\n",
    "X_train = np.random.randn(batch_size,max_seq_len, d_model) # 100 samples, each with d_model features\n",
    "y_train = np.random.randn(batch_size,max_seq_len,d_model)  # 100 target values\n",
    "\n",
    "\n",
    "\n",
    "ffn=TransfomerFF(d_model=d_model,max_seq_len=max_seq_len,units=units) \n",
    "ffn.init_weights(weights_mode=\"random\")\n",
    "\n",
    "\n",
    "_,epochs_hist_1=ffn.train_batch_sequence(X_train, y_train, epoch=num_epochs, learning_rate=learning_rate)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
