{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    # model parameters\n",
    "    def __init__(self,embedding_dict_path,d_model,max_seq_len,batch_size): \n",
    "        \n",
    "        self.d_model = d_model # the dimension of the embedding space\n",
    "        self.max_seq_len = 100\n",
    "        # read the embedding dictionary\n",
    "        # the file is a csv file with the first column as the word and the rest as the embedding vector\n",
    "        \"\"\"\n",
    "        embedding_dict={}\n",
    "        with open(embedding_dict_path,'r') as f:\n",
    "            # create a dictionary with the word as the key and the embedding vector as the value\n",
    "            # force the value to float64\n",
    "            for line in f:\n",
    "                elements=line.strip().split(',')\n",
    "                word=elements[0]\n",
    "                vector=np.array(elements[1:],dtype=np.float64)\n",
    "                embedding_dict[word] = vector\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.vocab_size = len(embedding_dict)\n",
    "        \"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "class InputEmbedding:\n",
    "    def __init__(self,bpe_tokenizer_path):\n",
    "        \"\"\"\n",
    "        Initialize the input embedding layer\n",
    "\n",
    "        input:\n",
    "        bpe_tokenizer json file path\n",
    "        \"\"\"\n",
    "        self.tokenizer = Tokenizer.from_file(bpe_tokenizer_path)\n",
    "    def __input_embedding__(self,inputs):\n",
    "        \"\"\"\n",
    "        Convert the input batch of strings to its corresponding embedding vector\n",
    "        \n",
    "        parameters:\n",
    "        inputs: input batch of strings\n",
    "        Returns:\n",
    "        numpy array of shape (batch_size,seq_len,d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        # iterate through the input batch and convert each string to its corresponding embedding vector\n",
    "        batch_sequences = []\n",
    "        for input in inputs:\n",
    "            # encode the input string\n",
    "            encoding = self.tokenizer.encode(input)\n",
    "            # get the embedding vector\n",
    "            embedding = encoding.ids\n",
    "            batch_sequences.append(embedding)\n",
    "        return np.array(batch_sequences)\n",
    "    def \n",
    "        \"\"\"\n",
    "        batch_sequences = []\n",
    "        for input in inputs:\n",
    "            single_sequence = []\n",
    "            for word in input:\n",
    "                if word not in embedding_dict:\n",
    "                    raise ValueError(f\"{word} not found in the embedding dictionary\")\n",
    "                single_sequence.append(embedding_dict[word])\n",
    "            batch_sequences.append(single_sequence)\n",
    "        # return a tensor of shape (batch_size,seq_len,d_model)\n",
    "        \"\"\"\n",
    "        #return np.array(batch_sequences)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 232,  148,   69, 2198],\n",
       "       [ 232,  148,   69, 2198]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_loaded=InputEmbedding(\"../data/bpe_tokenizer.json\")\n",
    "encoded_data=tokenizer_loaded.input_embedding([\"hello world\",\"hello world\"])\n",
    "encoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "d_model = 512\n",
    "data_dir = \"../data/word_embedding.csv\"\n",
    "Transformer_model=Transformer(data_dir,d_model,max_seq_len,batch_size)\n",
    "Transformer_model.embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self,seqlen,d_model,batch_size):\n",
    "        self.seq_len = seqlen\n",
    "        self.d_model = d_model\n",
    "        self.batch_size = batch_size\n",
    "        self._positional_encoding = None  # Cache for the positional encoding\n",
    "    def positional_encoding(self):\n",
    "        \"\"\"\n",
    "        create a positional encoding for the input sequence ( it is independent of the input sequence.\n",
    "        However, it is dependent on the length of the sequence) \n",
    "        This is computed once and then repeated for the batch size.\n",
    "        \n",
    "\n",
    "        Returns : \n",
    "        Tensor of shape (batch_size,seq_len,d_model) \n",
    "        \"\"\" \n",
    "        if self._positional_encoding is None:\n",
    "            # Compute positional encoding for a single sequence\n",
    "            pos_encoding = np.zeros((self.seq_len, self.d_model))\n",
    "            positions = np.arange(self.seq_len)[:, np.newaxis]\n",
    "            dimensions = np.arange(self.d_model)[np.newaxis, :]\n",
    "            angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float64(self.d_model))\n",
    "            angle_rads = positions * angle_rates\n",
    "\n",
    "            pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "            pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "            # Repeat for batch size and cache\n",
    "            self._positional_encoding = np.repeat(pos_encoding[np.newaxis, :, :], self.batch_size, axis=0)\n",
    "\n",
    "        return self._positional_encoding\n",
    "    def positional_encoding_single_sequence(self):\n",
    "        \"\"\"\n",
    "        create a positional encoding for the input sequence ( it is independent of the input sequence.\n",
    "        However, it is dependent on the length of the sequence) \n",
    "        \n",
    "        Parameters: \n",
    "        seq_len: the length of the sequence\n",
    "        Returns : \n",
    "        Tensor of shape (seq_len,d_model) \n",
    "        \"\"\"\n",
    "        pos_encodings = np.zeros((self.seq_len, self.d_model))\n",
    "\n",
    "\n",
    "        positions = np.arange(self.seq_len)[:, np.newaxis]\n",
    "        dimensions = np.arange(self.d_model)[np.newaxis, :]\n",
    "\n",
    "\n",
    "        # Compute angles, integer division of indices by 2 is used to fix a pair wise frequency (same frequency for even and odd indices)\n",
    "        angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float64(self.d_model))\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        # Apply sin and cos to odd and even indices\n",
    "        pos_encodings[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        pos_encodings[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        return pos_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/word_embedding.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m d_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      5\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/word_embedding.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m Transformer_model\u001b[38;5;241m=\u001b[39m\u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m Transformer_model\u001b[38;5;241m.\u001b[39membedding_dict\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, embedding_dict_path, d_model, max_seq_len, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# read the embedding dictionary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# the file is a csv file with the first column as the word and the rest as the embedding vector\u001b[39;00m\n\u001b[1;32m      9\u001b[0m embedding_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding_dict_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# create a dictionary with the word as the key and the embedding vector as the value\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# force the value to float64\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m         elements\u001b[38;5;241m=\u001b[39mline\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/word_embedding.csv'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
