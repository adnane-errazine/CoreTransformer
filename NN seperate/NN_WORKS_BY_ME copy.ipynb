{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:0.9987446478756103\n",
      "delta shape (100, 10)\n",
      "grad_bias2 shape (100, 10)\n",
      "self.bias2 shape (1, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,10) doesn't match the broadcast shape (100,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 198\u001b[0m\n\u001b[0;32m    194\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(max_seq_len,d_model)  \u001b[38;5;66;03m# 100 target values\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m#ffn.forward(X_train).shape\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m \u001b[43mffn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_without_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m ffn\u001b[38;5;241m.\u001b[39mpredict(X_train)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[8], line 181\u001b[0m, in \u001b[0;36mTransfomerFF.train_without_batch\u001b[1;34m(self, x, y, epoch, learning_rate)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    180\u001b[0m     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackprop_without_batch(x,y)\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mloss_to_epoch_history))\n\u001b[0;32m    183\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[8], line 155\u001b[0m, in \u001b[0;36mTransfomerFF.update_weights\u001b[1;34m(self, parameters, learning_rate)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mparameters[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mparameters[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,10) doesn't match the broadcast shape (100,10)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def mse_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error loss between the true and predicted values (the cost function)\n",
    "    \"\"\"\n",
    "    return np.mean(np.square(y_true-y_pred))\n",
    "    \n",
    "\n",
    "def derivative_mse_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the mean squared error loss function with respect to the predicted values\n",
    "    input:\n",
    "    y_true: numpy array, dimensions (batch_size,seq_len,d_model), the true values\n",
    "    y_pred: numpy array, dimensions (batch_size,seq_len,d_model), the predicted values\n",
    "    output:\n",
    "    numpy array, dimensions (batch_size,seq_len,d_model), the derivative of the loss function with respect to the predicted values \n",
    "    \"\"\"\n",
    "\n",
    "    return 2*(y_pred-y_true)/y_true.shape[0]\n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def Relu_derivative(x):\n",
    "    return np.where(x>0,1,0)\n",
    "class TransfomerFF:\n",
    "    def __init__(self,d_model=4,max_seq_len=100,units=2048,batch_size=32,name=\"FF_layer_X\"):\n",
    "            self.d_model = d_model\n",
    "            self.max_seq_len = max_seq_len\n",
    "            self.batch_size = batch_size\n",
    "            self.weights1 = None\n",
    "            self.weights2 = None\n",
    "            self.bias1 = None\n",
    "            self.bias2 = None\n",
    "            self.name = name\n",
    "            self.units = units # the number of units in the hidden layer\n",
    "    def init_weights(self,weights_mode=\"random\",weights1=None,weights2=None,bias1=None,bias2=None):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        weights_mode: str, the mode of the weights initialization, can be \"random\" or \"pretrained\"\n",
    "        weights1: numpy array, the weights of the first layer\n",
    "        weights2: numpy array, the weights of the second layer\n",
    "        bias1: numpy array, the bias of the first layer\n",
    "        bias2: numpy array, the bias of the second layer\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if weights_mode==\"random\":\n",
    "            self.weights1 = np.random.randn(self.d_model,self.units)*0.01\n",
    "            self.weights2 = np.random.randn(self.units,self.d_model)*0.01\n",
    "            self.bias1 = np.zeros((1, self.units)) #np.random.randn(1,self.units)\n",
    "            self.bias2 =  np.zeros((1, self.d_model)) #  np.random.randn(1,self.d_model)\n",
    "        elif weights_mode==\"pretrained\":\n",
    "            # check if the weights are provided and if they are of the correct shape\n",
    "            if (weights1 is None or weights2 is None or \n",
    "                bias1 is None or bias2 is None):\n",
    "                raise ValueError(\"weights1, weights2, bias1, and bias2 must be provided for pretrained mode.\")\n",
    "            assert weights1.shape == (self.d_model,self.units)\n",
    "            assert weights2.shape == (self.units,self.d_model)\n",
    "            assert bias1.shape == (1,self.units)\n",
    "            assert bias2.shape == (1,self.d_model)\n",
    "            \n",
    "            self.weights1 = weights1\n",
    "            self.weights2 = weights2\n",
    "            self.bias1 = bias1\n",
    "            self.bias2 = bias2\n",
    "            \n",
    "        elif weights_mode == \"null\":\n",
    "            self.weights1 = np.zeros((self.d_model,self.units))\n",
    "            self.weights2 = np.zeros((self.units,self.d_model))\n",
    "            self.bias1 = np.zeros((1,self.units))\n",
    "            self.bias2 = np.zeros((1,self.d_model))\n",
    "    def forward_old(self,inputs):\n",
    "        \n",
    "        # create the first hidden layer\n",
    "        z1 = np.dot(inputs,self.weights1) + self.bias1\n",
    "        \n",
    "        \n",
    "        a1 = np.maximum(0,z1)\n",
    "        \n",
    "        # create the second hidden layer\n",
    "        \n",
    "        z2 = np.dot(a1,self.weights2) + self.bias2\n",
    "        \n",
    "        return z2\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        # X shape: (batch_size, max_seq_len, d_model)\n",
    "        self.batch_size, self.max_seq_len, _ = inputs.shape\n",
    "        # Reshape X to 2D for easier computation\n",
    "        X_reshaped = inputs.reshape(-1, self.d_model)\n",
    "        # First layer\n",
    "        self.Z1 = np.dot(X_reshaped, self.weights1) + self.bias1\n",
    "        \n",
    "        # apply the activation function -  Leaky ReLU\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU activation\n",
    "            \n",
    "        # Second layer\n",
    "        Z2 = np.dot(self.A1, self.weights2) + self.bias2\n",
    "            \n",
    "        # Reshape output back to 3D\n",
    "        return Z2.reshape(self.batch_size, self.max_seq_len, self.d_model)\n",
    "    \"\"\"\n",
    "    def forward_without_batch(self,inputs):\n",
    "        # inputs shape: (seq_len,d_model)\n",
    "        z1 = np.dot(inputs,self.weights1) + self.bias1\n",
    "        a1 = Relu(z1)\n",
    "        z2 = np.dot(a1,self.weights2) + self.bias2\n",
    "        return z2\n",
    "    def backprop_without_batch(self,inputs,targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs: numpy array, dimensions (seq_len,d_model), the input data\n",
    "        targets: numpy array, dimensions (seq_len,d_model), the target values\n",
    "        \"\"\"\n",
    "        \n",
    "        # create the gradient arrays\n",
    "        grad_weights1 = np.zeros_like(self.weights1)\n",
    "        grad_weights2 = np.zeros_like(self.weights2)\n",
    "        grad_bias1 = np.zeros_like(self.bias1)\n",
    "        grad_bias2 = np.zeros_like(self.bias2)\n",
    "        \n",
    "        # feed forward pass\n",
    "        \n",
    "        z_layer_1 = np.dot(inputs,self.weights1) + self.bias1\n",
    "        a_layer_1 = Relu(z_layer_1)\n",
    "        \n",
    "        z_layer_2 = np.dot(a_layer_1,self.weights2) + self.bias2\n",
    "\n",
    "        # delta is gradient of the cost with respect to z at the output layer ( using BP1 formula)\n",
    "        \n",
    "        delta=derivative_mse_loss(targets,z_layer_2) \n",
    "        \n",
    "        grad_weights2 = np.dot(a_layer_1.T,delta) # or np.dot(delta,a_layer_1.T) TO VERIFY THE DIMENSIONS\n",
    "        \n",
    "        print(\"delta shape\",delta.shape)\n",
    "        #grad_bias2 = delta.sum(axis=0,keepdims=True)\n",
    "  \n",
    "        grad_bias2 = delta\n",
    "        print(\"grad_bias2 shape\",grad_bias2.shape)\n",
    "        print(\"self.bias2 shape\",self.bias2.shape)\n",
    "        \n",
    "        delta = np.dot(delta,self.weights2.T) * Relu_derivative(z_layer_1)\n",
    "        grad_weights1 = np.dot(inputs.T,delta)\n",
    "        grad_bias1 = delta.sum(axis=0,keepdims=True)\n",
    "        #grad_bias1 = delta\n",
    "        \n",
    "        return [grad_weights1,grad_weights2,grad_bias1,grad_bias2]\n",
    "        \n",
    "        \n",
    "    def update_weights(self,parameters,learning_rate=0.01):\n",
    "        self.weights1 -= learning_rate*parameters[0]\n",
    "        self.weights2 -= learning_rate*parameters[1]\n",
    "        self.bias1 -= learning_rate*parameters[2]\n",
    "        self.bias2 -= learning_rate*parameters[3]\n",
    "    \n",
    "    \n",
    "    #def forward_without_batch(self,inputs):\n",
    "    #    z1 = np.dot(inputs,self.weights1) + self.bias1\n",
    "    #    a1 = np.maximum(0.01*z1,z1)\n",
    "    #    z2 = np.dot(a1,self.weights2) + self.bias2\n",
    "    #    return z2\n",
    "    def predict(self,inputs):\n",
    "        return self.forward_without_batch(inputs)\n",
    "    def backprop_batch(self):\n",
    "        pass\n",
    "    def update_weights_batch(self):\n",
    "        pass\n",
    "    def train_batch(self):\n",
    "        pass\n",
    "    def train_without_batch(self,x,y,epoch=1001,learning_rate=0.1):\n",
    "        loss_to_epoch_history=[]\n",
    "        \n",
    "        for epoch in range(epoch):\n",
    "            pred=self.forward_without_batch(x)\n",
    "            loss=mse_loss(y,pred)\n",
    "            loss_to_epoch_history.append((epoch,loss))\n",
    "            if epoch%1000==0:\n",
    "                print(f\"Epoch:{epoch}, Loss:{loss}\")\n",
    "            params=self.backprop_without_batch(x,y)\n",
    "            self.update_weights(params,learning_rate)\n",
    "        plt.plot(*zip(*loss_to_epoch_history))\n",
    "        plt.show()\n",
    "        return [self.weights1,self.weights2,self.bias1,self.bias2],loss_to_epoch_history\n",
    "\n",
    "d_model=10\n",
    "max_seq_len=100\n",
    "units=128\n",
    "learning_rate=0.01\n",
    "\n",
    "ffn=TransfomerFF(d_model=d_model,max_seq_len=max_seq_len,units=units) \n",
    "ffn.init_weights(weights_mode=\"random\")\n",
    "X_train = np.random.randn(max_seq_len, d_model) # 100 samples, each with d_model features\n",
    "y_train = np.random.randn(max_seq_len,d_model)  # 100 target values\n",
    "\n",
    "\n",
    "#ffn.forward(X_train).shape\n",
    "ffn.train_without_batch(X_train, y_train, epoch=10001, learning_rate=learning_rate)\n",
    "ffn.predict(X_train).shape\n",
    "print(y_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
